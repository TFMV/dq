# DQOps Platform Overview

## Architecture and Core Components

**DQOps Runtime (Engine):** At the heart of DQOps is an all-in-one data quality engine (built mainly in Java with Spring Boot) that bundles multiple interfaces. This *DQOps runtime* includes an embedded web server (hosting a rich web UI and a REST API) as well as an interactive command-line shell for running data quality commands ([DQOps Data Quality Engine Architecture](https://dqops.com/docs/dqo-concepts/architecture/dqops-architecture/#:~:text=%2A%20%60DQOps%20runtime%60%20,quality%20engine%20that%20contains%20both)). In practice, you can run DQOps as a local service (e.g. via a PyPI package or Docker container) which then serves both the UI and CLI. The runtime is responsible for executing data quality jobs (like running checks) and exposing services to users and other tools.

**DQOps User Home (Local Storage):** All configuration and metadata are stored on the local file system in a designated folder (the “DQOps user home”), rather than in a relational database ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=DQOps%20does%20not%20use%20a,visible%20in%20the%20user%20interface)) ([DQOps Data Quality Engine Architecture](https://dqops.com/docs/dqo-concepts/architecture/dqops-architecture/#:~:text=integration%20with%20external%20tools%2C%20,in%20the%20Data%20storage%20documentation)). This folder contains YAML files defining data sources and the checks enabled, as well as a `.data/` subdirectory where results are stored as Parquet files. This design makes DQOps highly *DevOps/GitOps-friendly* – you can manage configurations (which act as data quality *data contracts*) in version-controlled YAML, and easily promote changes through environments or track history in Git ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=DQOps%20does%20not%20use%20a,visible%20in%20the%20user%20interface)). Non-technical users, however, can configure everything through the UI without touching YAML, and DQOps will update these files under the hood. The user home also caches all results locally, enabling offline or on-prem use. Key subfolders include: `sources/` (connections and table configs), `checks/`, `sensors/`, `rules/` (for any custom definitions), and `.data/` (for results data) ([How Data Quality Results are Stored? Examples and Best Practices](https://dqops.com/docs/dqo-concepts/data-storage-of-data-quality-results/#:~:text=5,intended%20to%20be%20modified%20manually)).

**Storage and Data Model:** DQOps stores each type of outcome in structured Parquet tables under `.data/`. For example, sensor measurements (raw metrics from data sources) are stored in `sensor_readouts/`, check evaluation outcomes go to `check_results/`, data quality issues are grouped into `incidents/`, execution errors (if any) in `errors/`, and optional error samples (rows that failed certain validations) in `error_samples/`. Basic profiling statistics (like distinct values, top values, etc.) are also stored under `statistics/` ([How Data Quality Results are Stored? Examples and Best Practices](https://dqops.com/docs/dqo-concepts/data-storage-of-data-quality-results/#:~:text=4,intended%20to%20be%20modified%20manually)). This local “data quality data lake” holds a historical record of data quality over time in a columnar format for efficiency. By default, one result per check per period is stored (e.g. one entry per day for daily checks, one per month for monthly profiling checks) – new results append or replace as appropriate ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=The%20sensor%20readouts%20are%20saved,replaced%20in%20the%20parquet%20table)) ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=The%20check%20results%20after%20validation,the%20partition%20day%20or%20month)).

**Check Execution Pipeline:** The DQOps engine executes data quality checks in a staged pipeline. When you trigger a “run checks” job (via CLI, UI button, API, or an internal scheduler), the job is placed on an internal queue and then processed stepwise ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=The%20possible%20methods%20for%20starting,a%20%27run%20checks%27%20job%20are)):

1. **Target Resolution:** The engine resolves which checks to run based on the target filters (e.g. a specific connection, schema, table name or check category). It reads the YAML configurations in the user home to find all checks that match the criteria (for example, “all daily monitoring checks on tables in schema X”) ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=2,of%20checks%20that%20are%20selected)).

2. **Sensor Query Rendering:** For each selected data quality check, the associated **sensor** (an SQL template in Jinja2) is rendered into a concrete SQL query against the target data source ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=3,column%20name%20and%20additional%20parameters)). The sensor definition fills in placeholders like table name, column, etc., producing a query that computes a certain metric. *Example:* a sensor for “null percentage” will generate a `SELECT ... COUNT(...)` query to compute the percent of NULLs in the column ([How Data Quality Results are Stored? Examples and Best Practices](https://dqops.com/docs/dqo-concepts/data-storage-of-data-quality-results/#:~:text=SELECT%20CASE%20WHEN%20COUNT%28,%2F%20COUNT)) ([How Data Quality Results are Stored? Examples and Best Practices](https://dqops.com/docs/dqo-concepts/data-storage-of-data-quality-results/#:~:text=1,by%20the%20data%20quality%20sensor)). DQOps has a library of built-in sensor definitions (with variations for different database providers), and you can add your own. The rendering happens in a Python Jinja2 engine, and if a custom sensor is present in the user home (overriding a built-in), it will use the custom one ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=The%20definitions%20of%20custom%20checks%2C,the%20custom%20definition%20is%20used)). (If the Jinja2 rendering fails due to a template error, the job logs an error entry in the `errors` table ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=4.%20,to%20the%20errors%20parquet%20table)) ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=4.%20,to%20the%20errors%20parquet%20table)).)

3. **Query Execution (Sensors):** The rendered SQL sensors are executed against the data source using JDBC or native drivers ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=errors%20parquet%20table)) ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=5.%20,Google%20BigQuery%20Java%20native%20libraries)). Each sensor query returns a result with at least an `actual_value` (the measured metric) and a `time_period` (timestamp or date representing the period of data it covers) ([How Data Quality Results are Stored? Examples and Best Practices](https://dqops.com/docs/dqo-concepts/data-storage-of-data-quality-results/#:~:text=DATE_TRUNC,time_period_utc%20ORDER%20BY%20time_period%2C%20time_period_utc)) ([How Data Quality Results are Stored? Examples and Best Practices](https://dqops.com/docs/dqo-concepts/data-storage-of-data-quality-results/#:~:text=2,the%20beginning%20of%20the%20month)). DQOps can optimize this step by *merging multiple sensors into one query* when they target the same table, to reduce load on the database ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=connectors%2C%20such%20as%20Google%20BigQuery,Java%20native%20libraries)) ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=It%20is%20worth%20mentioning%20that,scan%20of%20the%20tested%20table)). (For instance, instead of running 5 separate SELECTs on a table for 5 metrics, DQOps may combine them into one broader SELECT with multiple metrics in one scan ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=It%20is%20worth%20mentioning%20that,scan%20of%20the%20tested%20table)).) This is a deliberate design for efficiency on big data sets ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=DQOps%20supports%20incremental%20data%20quality,on%20the%20monitored%20data%20source)). If any database error occurs while executing the sensor (e.g. missing table or permission issue), that is also recorded in `errors` logs ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=performs%20a%20single,of%20the%20tested%20table)). The raw sensor results are then stored in the local `sensor_readouts` parquet, one entry per check *time series* per period ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=4,DQOps%20are%20called%20sensor%20readouts)) ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=sensor%20metrics%20that%20are%20captured,DQOps%20are%20called%20sensor%20readouts)) (ensuring historical metrics are tracked over time).

4. **Rule Evaluation:** Next, the engine evaluates **rules** on each sensor reading. A *data quality rule* in DQOps is a Python-based business logic that assesses the sensor’s output against expected thresholds or patterns ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=DQOps%20supports%20both%20defining%20custom,issue%20if%20the%20rule%20fails)). Each check ties a sensor to a rule. For example, a rule might declare that “null_percent should be <= 5% for WARNING, <= 10% for ERROR, else FAIL as FATAL.” Rules can be simple threshold comparisons or more advanced anomaly detection algorithms. DQOps will load the appropriate rule definition (built-in or custom `.dqorule.yaml` and its Python `evaluate_rule` function) and determine if historical data is needed for it ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=5,historical%20values%20of%20sensor%20readouts)). If the rule mode is “previous_readouts” (i.e. it needs a time series to detect anomalies or trends), the engine will fetch the recent history of that sensor’s past values (from `sensor_readouts`) according to the rule’s `time_window` setting ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=historical%20values%20of%20sensor%20readouts)). Then it calls the rule’s Python function, passing the current value and history (if required).

   Rules are evaluated for up to three severity levels per check: **fatal, error, warning** ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=DQOps%20calls%20the%20data%20quality,are%20evaluated)). DQOps will first apply the most severe threshold (fatal); if the metric violates that, the check outcome is marked as a failure at “fatal” level and it stops there ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=DQOps%20calls%20the%20data%20quality,are%20evaluated)) ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=separately%20for%20each%20configured%20severity,are%20evaluated)). If the fatal condition passes, it proceeds to evaluate the error-level threshold, and then warning. The first level at which the rule “fails” determines the severity of the issue. If all rule evaluations pass, the check is considered **successful** (no issue) ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=If%20the%20fatal%20severity%20rule,on%20the%20data%20quality%20dashboards)). This multi-level rule engine is a core design: it lets each check classify issues by severity (e.g., a 5% null rate might be just a warning, but 50% null is an error). The final **check result** (pass or fail with severity) is then written to the `check_results` parquet store ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=The%20check%20results%20after%20validation,the%20partition%20day%20or%20month)), appended daily or replaced for the period as appropriate (profiling checks replace the monthly result, etc.) ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=The%20check%20results%20after%20validation,the%20partition%20day%20or%20month)). If the rule evaluation itself encounters an error (for instance, a bug in a custom Python rule), that too would be logged in `errors` ([How Data Quality Results are Stored? Examples and Best Practices](https://dqops.com/docs/dqo-concepts/data-storage-of-data-quality-results/#:~:text=The%20sensor%20readouts%20are%20processed,are%20added)).

5. **Incident Management and Notification:** After obtaining the check results, DQOps automatically groups any new **data quality issues** into incidents. An “incident” represents a persistent data quality problem – DQOps groups similar failures over time so you don’t get spammed by the same recurring daily error. For example, if a “null percent too high” check fails every day for a week on the same column, it can be tracked as one ongoing incident. The grouping logic is configurable per data source (you can set how to uniquely group issues, e.g. by table or by check) ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=DQOps%20groups%20similar%20data%20quality,until%20the%20issue%20is%20resolved)) ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=incident%20%20that%20can%20be,until%20the%20issue%20is%20resolved)). If a check failure doesn’t match an existing open incident, a new incident record is created (with status Open); if it matches, the incident’s counter and timestamp are updated ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=Grouping%20data%20quality%20issues%20into,until%20the%20issue%20is%20resolved)). Incident records are stored in the `incidents` parquet table with details like statuses and timestamps ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=incident%20%20that%20can%20be,until%20the%20issue%20is%20resolved)). Finally, if any new incident was opened, DQOps can send out **notifications**. It supports webhook integrations (e.g. posting a JSON payload to Slack or other endpoints) to alert teams about new issues ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=8,incidents)). You can configure global or per-connection webhooks in the UI or YAML (for instance, route incidents from Finance data to a Finance Slack channel) ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=When%20new%20data%20quality%20incidents,webhooks%2C%20such%20as%20the%20Slack)). This automation loop (detection -> incident -> notification) helps integrate DQOps into operational workflows.

**Core Abstractions:** In summary, the architecture relies on a few key abstractions:

- **Connections & Data Sources:** represent the source systems or datasets (with JDBC info, etc.). Each connection has its own schedule and grouping settings ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=Once%20new%20tables%20are%20imported%2C,m)) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=,choose%20one%20of%20two%20subtabs)).
- **Sensors:** parameterized SQL templates that **collect a metric** from a target table/column ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=3,column%20name%20and%20additional%20parameters)). They are library-defined (and extensible) so that users don’t have to hand-write SQL for common checks. Sensors output an `actual_value` (the measured metric) and usually a `time_period` (like the date) for which it’s measured ([How Data Quality Results are Stored? Examples and Best Practices](https://dqops.com/docs/dqo-concepts/data-storage-of-data-quality-results/#:~:text=DATE_TRUNC,time_period_utc%20ORDER%20BY%20time_period%2C%20time_period_utc)) ([How Data Quality Results are Stored? Examples and Best Practices](https://dqops.com/docs/dqo-concepts/data-storage-of-data-quality-results/#:~:text=2,the%20beginning%20of%20the%20month)).
- **Rules:** small Python evaluators that **apply logic to sensor outputs** (with optional historical context) to decide if data meets quality criteria ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=5,historical%20values%20of%20sensor%20readouts)) ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=DQOps%20calls%20the%20data%20quality,are%20evaluated)). Rules encapsulate thresholds or anomaly detection algorithms. They produce a boolean outcome (pass/fail) and can classify severity.
- **Checks:** a **check** is essentially a pairing of a sensor + a rule (plus a target specification) ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=DQOps%20supports%20both%20defining%20custom,issue%20if%20the%20rule%20fails)). For example, a “daily nulls percent anomaly” check might use the “nulls_percent” sensor on a given column and the “anomaly_detection_1pct” rule to flag large changes ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=column%20daily%20not%20nulls%20percent,all%20values%20in%20a%20text)) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=column%20daily%20empty%20column%20found,For%20example%2C%20when%20a%20column)). Checks are the primary unit of configuration – you enable/disable checks on your tables as needed.
- **Targets:** every check applies to a specific target: either a whole table or an individual column (or sometimes a group/partition of data). For instance, a table-level check might be “row_count” on table X, while a column-level check targets a particular column of that table. DQOps organizes configurations in a hierarchy of connection → table → column, so you can attach checks at the appropriate level. “Partitioned” checks also target each partition of a table separately (e.g. check completeness per day partition) ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=)).

**User Interfaces:** DQOps provides multiple ways to interact with the platform ([DQOps Data Quality Engine Architecture](https://dqops.com/docs/dqo-concepts/architecture/dqops-architecture/#:~:text=,50)):

- The **Web UI** is a full management console for data quality. It lets you configure connections and checks, view results, incidents, dashboards, etc., making the platform accessible to non-coders. For example, data stewards can use the UI to set up checks without writing YAML or code ([DQOps Data Quality Engine Architecture](https://dqops.com/docs/dqo-concepts/architecture/dqops-architecture/#:~:text=The%20principal%20design%20idea%20behind,quality%20checks%20from%20the%20browser)).
- The **CLI** (`dqops` shell) allows running commands in a scripting or interactive manner for those who prefer terminal or want to integrate with shell scripts. Commands exist to run checks (`check run`), import metadata, etc. ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=When%20the%20data%20quality%20checks,on%20an%20internal%20job%20queue)).
- The **REST API** allows programmatic access to all operations. DQOps publishes an OpenAPI for running checks, retrieving results, etc., which the UI itself uses. This API can be called from external tools or CI/CD.
- The **Python Client** is a wrapper around the REST API, provided as a `dqops` PyPI package ([DQOps Data Quality Engine Architecture](https://dqops.com/docs/dqo-concepts/architecture/dqops-architecture/#:~:text=Studio%20Community%20Connector.%20,The%20client%20supports)). This client makes it easy to call DQOps from within Python data pipelines or Airflow/Dbt jobs ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=Configure%20data%20quality%20checks%20in,code%2C%20with%20code%20completion)). For example, you can instantiate a `Client(base_url)` and then trigger a `run_checks()` with a filter to validate data after loading it in a pipeline ([GitHub - dqops/dqo: Data Quality and Observability platform for the whole data lifecycle, from profiling new data sources to full automation with Data Observability. Configure data quality checks from the UI or in YAML files, let DQOps run the data quality checks daily to detect data quality issues.](https://github.com/dqops/dqo#:~:text=Now%2C%20you%20can%20call%20operations,are%20already%20registered%20in%20DQOps)) ([GitHub - dqops/dqo: Data Quality and Observability platform for the whole data lifecycle, from profiling new data sources to full automation with Data Observability. Configure data quality checks from the UI or in YAML files, let DQOps run the data quality checks daily to detect data quality issues.](https://github.com/dqops/dqo#:~:text=full_table_name%3D%27sample_schema)).
- There are also **Airflow operators** available, simplifying integration with Apache Airflow DAGs (so you can add a task to run DQOps checks as part of a workflow) ([DQOps Data Quality Engine Architecture](https://dqops.com/docs/dqo-concepts/architecture/dqops-architecture/#:~:text=,50)).

**Deployment Options:** DQOps is designed to run *locally* for development or as a *server* in production. Locally, you can simply install via `pip install dqops` and launch it (`python -m dqops`), which will auto-download the needed JRE and DQOps engine binaries ([DQOps Data Quality Engine Architecture](https://dqops.com/docs/dqo-concepts/architecture/dqops-architecture/#:~:text=The%20simplest%20way%20to%20start,as%20a%20dqops%20PyPi%20package)). In production, it can be deployed via a Docker container (the team provides a Docker image) or hosted on a VM. DQOps also offers a managed **DQOps Cloud** service, mainly to host the data quality dashboards and cloud storage (discussed below). The architecture supports hybrid deployments – e.g. run the engine on-prem but sync results to the cloud for dashboarding ([DQOps Data Quality Engine Architecture](https://dqops.com/docs/dqo-concepts/architecture/dqops-architecture/#:~:text=Internally%2C%20DQOps%20distribution%20uses%20another,the%20DQOps%20home%20source%20folder)).

**DQOps Cloud (Dashboards Backend):** While the core engine and storage can run fully on-premise, DQOps Cloud is an optional component that provides a *Data Quality Data Lake and Warehouse* for analysis and visualization ([DQOps Data Quality Engine Architecture](https://dqops.com/docs/dqo-concepts/architecture/dqops-architecture/#:~:text=%2A%20%60DQOps%20Cloud%60%20,which%20has%20the%20following%20components)). When you sign up for a free DQOps Cloud account, the platform can sync your local results (parquet files) to a cloud bucket (the “data lake”), and then load them into a **BigQuery** dataset (the “data warehouse”) ([DQOps Data Quality Engine Architecture](https://dqops.com/docs/dqo-concepts/architecture/dqops-architecture/#:~:text=,using%20a%20dedicated%20DQOps%20Looker)) ([DQOps Data Quality Engine Architecture](https://dqops.com/docs/dqo-concepts/architecture/dqops-architecture/#:~:text=,DQOps%20user%20interface%2C%20enabling%20complex)). DQOps provides built-in **Looker Studio dashboards** that connect to that warehouse to display trends, KPIs, and incident reports ([DQOps Data Quality Engine Architecture](https://dqops.com/docs/dqo-concepts/architecture/dqops-architecture/#:~:text=incidents,The%20client%20supports)) ([DQOps Data Quality Engine Architecture](https://dqops.com/docs/dqo-concepts/architecture/dqops-architecture/#:~:text=,the%20Data%20Quality%20Data%20Warehouse)). These dashboards (covering data quality scorecards, trend lines of metrics, etc.) can be viewed directly in the DQOps UI under a Dashboards section. Essentially, every user gets a private BigQuery-backed repository of their data quality results (with multi-tenant isolation). This cloud sync is not mandatory, but it unlocks rich visual reporting for enterprise overview of data quality. (You can also configure custom dashboards on the data if desired ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=DQOps%20measures%20data%20quality%20with,supports%20custom%20data%20quality%20dashboards)).)

In summary, DQOps’s architecture balances **DevOps-friendly design** (file-based config, version control, easy automation) with an **easy UI** (for analysts and stewardship teams) ([DQOps Data Quality Engine Architecture](https://dqops.com/docs/dqo-concepts/architecture/dqops-architecture/#:~:text=The%20principal%20design%20idea%20behind,quality%20checks%20from%20the%20browser)). Under the hood, the core engine orchestrates a pipeline of sensor queries and rule evaluations, storing detailed results for analysis. A job scheduler and incident manager are built-in, so it can operate as a standalone monitoring service. Yet, all functionalities are accessible via API/CLI for integration into broader data workflows.

## Platform Data Flow (From Ingestion to Reporting)

**1. Connection Setup:** To begin using DQOps, you first register a data source connection. This can be done in the UI (by providing connection details in a form) or by writing a YAML file. In the **UI workflow** (as documented in the Getting Started guide), you’d navigate to *Data Sources* and “Add Connection,” then fill in details like connection name, data source type (e.g. PostgreSQL, Snowflake, CSV), and credentials ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=,Review%20scheduling)) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=After%20installation%20and%20starting%20DQOps%2C,monitoring%20and%20run%20basic%20statistics)). This creates a `connection.dqoconnection.yaml` file in the `sources/<connection>/` directory with those parameters. At this point, you have an empty connection set up.

**2. Metadata Import:** After defining the connection, the next step is to import the metadata (list of schemas, tables, and columns) from that data source into DQOps. In the UI, you would use the *Schemas* tab under the connection and select which schemas/tables to import ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=,for%20columns%20in%20this%20connection)). DQOps will introspect the source and create YAML files for each table (e.g. `<table>.dqotable.yaml`) that include the table’s structure (columns, data types) and a placeholder for checks. Essentially, this brings the tables under DQOps management. Once imported, these tables appear in the UI’s navigation tree.

**3. Initial Data Profiling (Basic Statistics):** Upon importing tables, DQOps immediately (or on command) gathers basic data statistics for each table and column. In fact, DQOps simplifies initial *data profiling* by automatically activating some default profiling checks and statistics collectors, which can be run to assess the new data ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=Once%20new%20tables%20are%20imported%2C,m)). For example, you will have statistics like min/max values, distinct counts, top 10 values, null counts, etc., collected for each column. In the UI, you can click “Start profiling” on the connection or table, which triggers the collection of these basic stats and the execution of default profiling checks ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=Upon%20import%2C%20you%20will%20receive,button%20to%20initiate%20this%20process)). By default, DQOps comes with a **data quality policy** for new tables that includes a set of low-impact profiling checks (run monthly) and monitoring checks (run daily) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=Once%20new%20tables%20are%20imported%2C,m)). As soon as you click the profiling button, DQOps runs those checks on the new data.

- *Default Profiling Checks:* These are meant to evaluate the data’s state and content. For example, DQOps might automatically run checks to detect if any text columns contain email addresses or US phone numbers (as a way of identifying PII or specific patterns) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=Target%20Check%20name%20Description%20column,Detects%20emails%20inside%20text%20columns)) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=Target%20Check%20name%20Description%20column,Detects%20emails%20inside%20text%20columns)). The default profile policy includes checks like “**profile contains USA phone percent**” and “**profile contains email percent**” on each text column ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=Target%20Check%20name%20Description%20column,Detects%20emails%20inside%20text%20columns)) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=column%20profile%20contains%20usa%20phone,Detects%20emails%20inside%20text%20columns)). These profile checks scan a sample of the data to see if, say, >0% of values resemble an email – if so, it flags that finding (not necessarily as an “error” but as an observation). Profiling checks typically produce a one-time or monthly result that helps you understand the data’s contents and quality baseline.

- *Basic Statistics:* In addition to formal checks, DQOps collects summary statistics for each column (accessible in the *Basic Statistics* tab in the UI). This includes row counts, distinct counts, null counts, max/min, average lengths of text, etc. ([Review results from data quality assessment and automatically configure data quality checks](https://dqops.com/docs/getting-started/review-results-and-run-monitoring-checks/#:~:text=Review%20basic%20statistics%20results)). These statistics don’t have pass/fail outcomes; they are descriptive. They are also used by DQOps internally for the next step (rule mining). After the profiling run, you can browse these stats in the UI to get a sense of your data. For example, you might sort columns by % nulls to see which columns are mostly empty ([Review results from data quality assessment and automatically configure data quality checks](https://dqops.com/docs/getting-started/review-results-and-run-monitoring-checks/#:~:text=1,the%20Basic%20data%20statistics%20tab)).

**4. Reviewing Profiling Results:** Once the initial profiling job completes, you can examine what DQOps found. In the UI’s *Profiling* section, selecting a table shows a “Table quality status” summarizing how many checks passed/failed for that table in the profiling category. You can drill down to each check’s result. For instance, if the “contains email” check found emails in a column, it might show a warning or simply a percentage. DQOps also allows viewing **error samples** for certain checks – for example, if a validity check fails for some rows, you can see sample records that failed (this is particularly for data validity checks that collect bad rows) ([Review results from data quality assessment and automatically configure data quality checks](https://dqops.com/docs/getting-started/review-results-and-run-monitoring-checks/#:~:text=,View%20Tables%20summary)). At this stage, since these are profiling checks, you’re not necessarily alerting anyone – you’re exploring the data quality landscape of the new source.

**5. Automated Rule Inference (Rule Mining):** A standout feature of DQOps is its **Rule Mining engine**, which helps you configure recurring data quality checks based on the profiling results. Instead of manually deciding thresholds for every column, you can use rule mining to “infer” sensible rules from the data itself ([What is Data Quality Rule Mining? Examples and Best Practices](https://dqops.com/docs/dqo-concepts/data-quality-rule-mining/#:~:text=DQOps%20makes%20this%20process%20automatic,known%20email%20format)). The idea is to leverage the statistical profile and a sample of data to propose checks that would catch anomalies or outliers in that dataset. In the UI, there is a *Data Quality Rule Mining* tab under Profiling for each table ([Review results from data quality assessment and automatically configure data quality checks](https://dqops.com/docs/getting-started/review-results-and-run-monitoring-checks/#:~:text=To%20navigate%20to%20the%20data,quality%20rule%20miner)). When you open it, DQOps automatically proposes a list of potential checks and rule thresholds, tailored to the data you just profiled ([Review results from data quality assessment and automatically configure data quality checks](https://dqops.com/docs/getting-started/review-results-and-run-monitoring-checks/#:~:text=The%20rule%20mining%20screen%20allows,the%20proposed%20check%20configuration)).

   For example, if a column currently has 2% NULLs, the rule miner might propose a **monitoring check** that triggers if NULLs exceed, say, 5% (assuming you only want a small increase to be flagged) ([Review results from data quality assessment and automatically configure data quality checks](https://dqops.com/docs/getting-started/review-results-and-run-monitoring-checks/#:~:text=Setting%20up%20data%20quality%20rules,in%20data%20distribution%20over%20time)) ([Review results from data quality assessment and automatically configure data quality checks](https://dqops.com/docs/getting-started/review-results-and-run-monitoring-checks/#:~:text=configuration%20of%20data%20quality%20checks,the%20proposed%20check%20configuration)). It looks at the distribution and suggests boundaries that would catch anomalies without false alarms. You can adjust a sensitivity parameter (error rate %) to control how aggressive the suggestions are ([Review results from data quality assessment and automatically configure data quality checks](https://dqops.com/docs/getting-started/review-results-and-run-monitoring-checks/#:~:text=,quality%20checks%20to%20detect%20data)). The proposals are grouped by table and column, and you can expand each to see details. DQOps not only proposes checks to catch current issues, but also **baseline checks** that currently pass – these baseline checks serve as drift monitors so that if the data changes in the future (e.g., a column’s pattern shifts), they will fail ([What is Data Quality Rule Mining? Examples and Best Practices](https://dqops.com/docs/dqo-concepts/data-quality-rule-mining/#:~:text=Besides%20configuring%20checks%20to%20find,at%20the%20time%20of%20profiling)). This approach means you get a mix of “currently failing” and “currently passing” checks, both of which are useful: the former flag known issues, the latter guard against future deviations ([What is Data Quality Rule Mining? Examples and Best Practices](https://dqops.com/docs/dqo-concepts/data-quality-rule-mining/#:~:text=engine,known%20email%20format)) ([What is Data Quality Rule Mining? Examples and Best Practices](https://dqops.com/docs/dqo-concepts/data-quality-rule-mining/#:~:text=Besides%20configuring%20checks%20to%20find,at%20the%20time%20of%20profiling)).

   After reviewing the suggestions, you select which checks to apply and hit **Apply**. This writes the configuration of those checks into the YAML for that table (or multiple tables) ([Review results from data quality assessment and automatically configure data quality checks](https://dqops.com/docs/getting-started/review-results-and-run-monitoring-checks/#:~:text=The%20rule%20mining%20screen%20allows,the%20proposed%20check%20configuration)) ([Review results from data quality assessment and automatically configure data quality checks](https://dqops.com/docs/getting-started/review-results-and-run-monitoring-checks/#:~:text=configuration%20of%20data%20quality%20checks,the%20proposed%20check%20configuration)). Essentially, rule mining automates the heavy lifting of defining dozens of column checks across a dataset, whereas in other tools you’d have to manually write each expectation. At this point, you have a suite of data quality checks configured for ongoing monitoring.

**6. Ongoing Monitoring Checks Execution:** Once checks are configured (either via rule mining or manually), you will start executing them on a regular basis. DQOps supports both manual triggering and scheduled runs:

- *Manual Run:* You can always run checks on-demand. For example, you might use the CLI command `dqo check run --connection <name> --table <table>` to execute all checks on a table immediately ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=The%20possible%20methods%20for%20starting,a%20%27run%20checks%27%20job%20are)). In the UI, you can similarly trigger a run for a table or the entire connection (there’s a “Run checks” button). This will enqueue a job and run through the pipeline described earlier (sensor → rule → incident). The results will be viewable in the UI a moment later.

- *Scheduling:* For automation, DQOps uses an internal CRON-based scheduler. By default, when you created the connection, DQOps likely set up a default schedule (which can be seen in the *Schedule* tab of the connection settings) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=To%20review%20scheduling%20for%20profiling,and%20daily%20monitoring%20checks)) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=,choose%20one%20of%20two%20subtabs)). Typically, **profiling checks** are scheduled monthly (e.g., the 1st of the month at noon) and **daily monitoring checks** are scheduled daily (e.g., every day at 12:00) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=anomalies%2C%20empty%20tables%2C%20table%20availability%2C,m)). These default CRON schedules were mentioned when the connection was created ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=Once%20new%20tables%20are%20imported%2C,m)) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=anomalies%2C%20empty%20tables%2C%20table%20availability%2C,m)). You can adjust these schedules or disable/enable them in the UI. For instance, an enterprise might schedule critical table checks to run every hour. The scheduler will automatically trigger `run checks` jobs at those times in the background (you don’t need an external orchestrator, though you can integrate one if preferred). If the DQOps instance is running continuously, it will execute checks as per the CRON expressions.

   Each monitoring run will produce new sensor readings and check results. For daily checks, each day’s results append to the history ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=The%20check%20results%20after%20validation,the%20partition%20day%20or%20month)), building up time-series of data quality metrics. DQOps’s merging logic means if you have many checks on one table, it tries to query them efficiently together, minimizing impact on your data source even as you monitor numerous quality metrics ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=DQOps%20supports%20incremental%20data%20quality,on%20the%20monitored%20data%20source)).

**7. Results Analysis and Incident Tracking:** After running monitoring checks, you review the results. In the DQOps UI *Data Quality* section, you can see a **Table Quality Summary** for each table (for each day). This typically shows a status (e.g., “5/20 checks failed”) and possibly a color-coded score. You can click into a table to see which checks failed and at what severity. For example, you might see that “Daily row count anomaly” on `orders.csv` raised a warning yesterday (perhaps indicating a slight drop in row count) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=Target%20Check%20name%20Description%20table,count%20is%20greater%20than%2010)), or that “Daily column count changed” on a table triggered an error (meaning the schema changed unexpectedly) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=credentials%20to%20data%20sources,the%20percentage%20of%20null%20values)) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=the%20data%20source%20and%20counts,the%20percentage%20of%20null%20values)). Each check result can be inspected in a check detail screen, which will show the actual measured value vs the expected range. For instance, if a freshness check failed, it might show that the table’s last update time was 2 days ago which violated the freshness threshold of 1 day ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=row%20count%20is%20greater%20than,counts%20the%20number%20of%20columns)) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=table%20daily%20freshness%20anomaly%20,counts%20the%20number%20of%20columns)).

   The **Incidents** section (or tab) will highlight any new incidents created. If a new data quality issue was detected, you will find an incident opened with details on when it started, what check and data it involves, and how many times it’s occurred ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=incident%20%20that%20can%20be,until%20the%20issue%20is%20resolved)). For example, if your “null percent” check on `customer.email` column started failing, you might have an incident “High null percentage in customer.email” opened on Jan 10, and it will list each day it has occurred since. DQOps allows you to change incident status (Acknowledged, Resolved, Muted, etc.) via the UI, helping you manage the life cycle of data quality issues as you address them.

   If notifications were set up, your team might also receive, say, a Slack alert for the new incident with a JSON payload describing the issue ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=8,incidents)). This can be integrated into ticketing systems or simply to page on-call data engineers if a critical data table breaks a quality rule.

**8. Reporting and Dashboards:** Beyond the detailed results, DQOps provides high-level **Data Quality Dashboards** for trend analysis and KPI tracking. After you have run checks (especially profiling checks which compute an initial score), you likely want to see the overall data quality *score* or trend over time. DQOps measures data quality in terms of **KPIs** – for example, the percentage of checks passing, or specific dimension scores (completeness, validity, etc.). These are aggregated in the Looker Studio dashboards.

   To use the dashboards, you typically need to **synchronize** your results to the DQOps Cloud (unless you have set up your own equivalent). In the UI, there’s a “Synchronize” button – clicking it will upload the latest local results to your cloud data lake/warehouse ([Review data quality results on data quality dashboards](https://dqops.com/docs/getting-started/review-results-on-dashboards/#:~:text=To%20display%20results%20on%20data,corner%20of%20the%20navigation%20bar)). Once synced, you can open the *Dashboards* section. DQOps offers several built-in dashboards (Current Data Quality Status, Data Quality KPI Trends, Validity Issues, etc.) ([Review data quality results on data quality dashboards](https://dqops.com/docs/getting-started/review-results-on-dashboards/#:~:text=In%20DQOps%2C%20you%20can%20choose,in%20the%20DQOps%20concepts%20section)) ([Review data quality results on data quality dashboards](https://dqops.com/docs/getting-started/review-results-on-dashboards/#:~:text=In%20DQOps%2C%20you%20can%20choose,in%20the%20DQOps%20concepts%20section)). For example, the *Current Column Status* dashboard might show a bar chart of all columns with issues by category ([Review data quality results on data quality dashboards](https://dqops.com/docs/getting-started/review-results-on-dashboards/#:~:text=,Data%20sources)), and the *Data Quality KPIs* dashboard will show an overall score for each table or data source (often calculated as % of checks passing or some weighted metric) ([Review data quality results on data quality dashboards](https://dqops.com/docs/getting-started/review-results-on-dashboards/#:~:text=,Use%20cases%20and%20examples)). These dashboards allow business stakeholders and executives to get a quick sense of data health without digging into technical details. They can answer questions like “Which tables are currently having issues and of what severity?” and “Are things getting better or worse over the last quarter in terms of data quality?” – all in a visual format. For instance, you might see that your data quality KPI for the Sales dataset is 95% this week, up from 90% last month as issues were resolved.

   It’s worth noting that if you cannot or prefer not to use the cloud dashboards, you still have the raw data in Parquet locally. Some users might build their own reporting (e.g., connect a BI tool to the Parquet files or export them to another database). But the provided dashboards are a convenient out-of-the-box solution. Each DQOps Cloud free account includes a hosted BigQuery and Looker Studio access to these standard dashboards ([Review data quality results on data quality dashboards](https://dqops.com/docs/getting-started/review-results-on-dashboards/#:~:text=Complimentary%20data%20quality%20data%20warehouse,for%20FREE%20accounts)).

**End-to-End Flow Recap:** To illustrate a typical flow: a data engineer connects a new Snowflake database to DQOps and imports a few tables. They run profiling checks and see that one column has inconsistent values. Using rule mining, they quickly configure a set of daily checks on all key columns. DQOps then monitors those each day. One morning, it detects an anomaly (say, a sudden drop in row count in a fact table) and raises an incident. The engineer gets a Slack alert, checks DQOps, and finds that the ETL job possibly failed to load all rows. They resolve the underlying issue, mark the incident as resolved, and next day the checks pass. Over time, the data quality KPI on the dashboard improves as fewer incidents occur. Throughout this process, data moves seamlessly from the source systems, through DQOps’s sensors and rules, into result stores, and finally into actionable reports and notifications.

## Comparison to Other Data Quality Tools (Great Expectations, Soda, etc.)

DQOps is part of the same family of data quality platforms as **Great Expectations (GE)** and **Soda Data** (Soda Core/Cloud), but it differentiates itself in several ways in terms of features, philosophy, and workflow:

- **Configuration and User Experience:** Great Expectations is primarily code-centric – you write Python or YAML to define *expectations* (tests) and typically run them in notebooks or pipelines. Soda Core uses a YAML DSL (SodaCL) where you declare checks in text files and run via CLI. **DQOps offers a hybrid approach**: it stores definitions in YAML (like Soda) for Git-based management ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=DQOps%20does%20not%20use%20a,visible%20in%20the%20user%20interface)), but also provides a rich UI to configure and view checks, similar to enterprise data quality tools. This means that less technical users can point-and-click to set up checks ([DQOps Data Quality Engine Architecture](https://dqops.com/docs/dqo-concepts/architecture/dqops-architecture/#:~:text=The%20principal%20design%20idea%20behind,quality%20checks%20from%20the%20browser)), whereas with GE or open-source Soda, a user often needs to manually code or edit text files. DQOps essentially bridges DataOps and UI-driven usability in one platform.

- **Architecture and Deployment:** All three tools can be integrated into data pipelines, but their form-factors differ. GE is a library that you embed in your Python code or notebooks; it doesn’t run as a standalone server (unless you count GE Cloud which is separate). Soda Core is a CLI/Agent you run to scan data (and optionally send results to Soda’s cloud service). **DQOps runs as a service/engine** – you start the DQOps process and it orchestrates checks, scheduling, etc., more like a continually running application. This leads to DQOps having built-in scheduling and incident management, whereas with GE or Soda you’d typically rely on external schedulers or don’t have an incident concept out-of-the-box. DQOps’s built-in job scheduler (Cron-based) ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=calling%20the%20run%20checks%20operation)) and alerting system means it can function autonomously to monitor data daily, which is closer to a *data observability* platform, not just a testing framework.

- **Data Quality Philosophy:** Great Expectations focuses on *asserting expectations* about data – you specify exactly what condition should hold (e.g., “no nulls in column X”, “values in range”). It’s very explicit and test-like. Soda’s checks are similar assertions (with some ability to do anomaly checks via syntax). **DQOps embraces a Data Observability philosophy**: it includes many automated anomaly detections, volume and schema drift checks, and supports the idea of monitoring data continuously to spot unexpected changes. For example, DQOps by default includes anomaly checks like “daily row count anomaly” (detect if today’s row count is a top 1% outlier compared to history) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=according%20to%20default%20Table%20and,level%20data%20quality%20policies)) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=Target%20Check%20name%20Description%20table,count%20is%20greater%20than%2010)) – this is something you’d have to custom-build in Great Expectations (GE has expectation for column values to be between an interval, but not a built-in “anomaly” logic without coding) and in Soda you could write a change check but not with an automated threshold. DQOps provides these “observability” checks out-of-the-box, including schema change detection (did a column disappear or its type change?) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=quality%20issue%20when%20the%20type,last%20time%20it%20was%20retrieved)), freshness checks (is data delayed?) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=row%20count%20is%20greater%20than,counts%20the%20number%20of%20columns)), and even cross-table comparisons for data reconciliation ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=)), which go beyond the scope of Great Expectations. In short, DQOps aims to catch *unexpected issues* with minimal user-defined thresholds, whereas GE expects the user to define *expected conditions*. This makes DQOps powerful for unknown unknowns and drift detection.

- **Automated Check Suggestions:** DQOps’s rule mining for auto-generating checks is a unique feature. Neither Great Expectations nor Soda Core has an equivalent mechanism that proposes tests based on data profiling. (Great Expectations does have a *“Profiler”* or *“Data Assistant”* in newer versions that can create initial expectations, but it’s more template-driven and not as focused on inferring thresholds from data distribution as DQOps’s rule miner is.) ([What is Data Quality Rule Mining? Examples and Best Practices](https://dqops.com/docs/dqo-concepts/data-quality-rule-mining/#:~:text=DQOps%20makes%20this%20process%20automatic,known%20email%20format)) ([What is Data Quality Rule Mining? Examples and Best Practices](https://dqops.com/docs/dqo-concepts/data-quality-rule-mining/#:~:text=Besides%20configuring%20checks%20to%20find,at%20the%20time%20of%20profiling)) Soda’s open-source offering doesn’t auto-suggest checks at all – you must write them. This means DQOps can drastically cut down the setup time by mining the data for you and suggesting what to monitor.

- **Separation of Sensor and Rule:** Internally, DQOps separates the metric collection (sensor SQL) from the validation logic (rule in Python). Great Expectations and Soda checks typically combine these – e.g., a Great Expectation both computes a metric and checks it in one code object. The DQOps design allows reuse of the same metric for different rules or easy extension of rules. For instance, you could use a “distinct_count” sensor and apply either a simple threshold rule or a statistical anomaly rule to it, without writing new SQL. This modularity is inspired by tools like Amazon Deequ, and is less evident in GE (though GE metrics can be reused in custom expectations to some extent). For users, this means DQOps might feel more configurable (you can mix and match sensors and rules or write new ones) whereas GE might require writing a new expectation class for a new type of logic.

- **Results Storage and History:** Great Expectations typically produces validation results as JSON files or in-memory objects; you can configure a *Validation Store* or use Data Docs to see a snapshot of results, but it doesn’t maintain a time-series of past runs out-of-the-box – you’d have to push results to an external store. Soda Core can output scan results to a file or to Soda Cloud, but again the open source by itself isn’t maintaining a historical store. **DQOps automatically builds a history** of sensor readings and check outcomes in its `.data` store ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=4,DQOps%20are%20called%20sensor%20readouts)) ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=The%20sensor%20readouts%20are%20saved,replaced%20in%20the%20parquet%20table)). This is significant: DQOps treats data quality measurements as time-series data, enabling trend analysis over time (either via its dashboards or by querying the Parquet). Great Expectations focuses more on pass/fail per batch, and while you can aggregate that, it’s not inherently time-series oriented in the open version. Soda Cloud does provide some historical tracking and charts, but that’s a proprietary service. DQOps provides the historical persistence in the open-source engine itself.

- **Incident Management and Collaboration:** DQOps has a concept of incidents and workflows (acknowledge, resolve, etc.) built in ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=DQOps%20groups%20similar%20data%20quality,until%20the%20issue%20is%20resolved)) ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=incident%20%20that%20can%20be,until%20the%20issue%20is%20resolved)), plus native Slack/webhook integration for alerts ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=8,incidents)). Great Expectations, in contrast, leaves alerting to the user – you’d integrate with PagerDuty or Slack manually in your pipeline if needed. Soda Core similarly would rely on external integration or using Soda Cloud’s alerting. So for teams, DQOps offers a more turnkey solution to not only detect but also manage data quality issues (almost like a lightweight ticketing system for data issues). This is aligned with its “operations center” philosophy: it’s not just testing, it’s monitoring and issue tracking in one.

- **Feature Breadth:** All three tools cover basic data quality dimensions like completeness (nulls), validity, uniqueness, etc. DQOps and Great Expectations both have extensive libraries of predefined checks/expectations. DQOps specifically emphasizes certain unique capabilities: **partitioned checks** (grouping by a dimension or analyzing each day partition separately) ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=)), **data stream segmentation** (monitoring metrics broken down by e.g. supplier or region columns up to 9 grouping levels) ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=)), and **cross-database comparisons** (like ensure a summary table in one DB matches row counts of a fact table in another) ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=)). These are fairly advanced scenarios – Great Expectations can be made to do some of this (with custom Python expectations or by writing custom queries), but DQOps has built-in support and UI for these cases. Soda’s open source is more limited in check types (mostly basic metrics and SQL checks), relying on Soda Cloud to extend functionality.

- **Integrations and Ecosystem:** Great Expectations has a large community and many integrations (Airflow operators, Prefect, dbt, etc.); Soda also integrates with Airflow and has some community. DQOps is newer on the scene, but it has focused on integration through its Python client and REST API. It provides Airflow operators out-of-the-box for its checks ([DQOps Data Quality Engine Architecture](https://dqops.com/docs/dqo-concepts/architecture/dqops-architecture/#:~:text=,50)) and demonstrates integration with dbt (for example, running DQOps after a dbt run, or treating DQOps checks as data tests in CI) ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=Configure%20data%20quality%20checks%20in,code%2C%20with%20code%20completion)). DQOps’s emphasis on YAML config in Git and the ability to run it headlessly make it *CI/CD friendly* similar to how GE expectations can be stored in Git. One difference: Great Expectations has no native UI (unless you use GE Cloud), whereas DQOps gives you a UI and optionally cloud dashboards for free – a compelling value for teams that don’t want to build their own front-end for results.

- **Philosophy of Use:** Great Expectations often is used in development or QA phases – e.g., a data engineer writing expectations to test a data pipeline. Soda too is often used to test data as it flows (especially if not using their cloud). **DQOps is geared slightly more toward continuous *monitoring in production***. The notion of schedules, incidents, and month-over-month KPI improvement ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=No%20data%20quality%20project%20can,that%20they%20understand%20and%20trust)) ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=DQOps%20measures%20data%20quality%20with,supports%20custom%20data%20quality%20dashboards)) is akin to running a data quality program over time, rather than one-off tests. In that sense, DQOps crosses into the territory of *Data Observability platforms*, which typically are commercial (Monte Carlo, etc.), whereas Great Expectations stays firmly in the *Data Testing* paradigm.

In summary, Great Expectations excels in flexible, fine-grained expectations and has a mature community, while Soda is lightweight and focuses on a checks DSL with cloud support. DQOps sets itself apart by providing an end-to-end **operations-focused** solution: it not only defines and runs checks, but also schedules them, stores their history, triages issues, and presents metrics on dashboards – all in an open platform. This comprehensive approach (plus features like automatic rule suggestions and SQL query optimization) ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=DQOps%20supports%20incremental%20data%20quality,on%20the%20monitored%20data%20source)) can significantly reduce the manual effort to implement a full data quality monitoring workflow compared to assembling pieces from GE or Soda.

## Key Concepts and Configuration Model

**Sensors (Data Quality Sensors):** A sensor in DQOps is the component that *measures* something about your data. Technically, it’s a Jinja2 SQL template that, when rendered and executed, returns a numeric result (and time period). Sensors capture raw metrics like “row count of a table”, “null percentage in a column”, “max value of a column”, “number of duplicate entries”, etc. Each sensor is defined once (in YAML) with parameters for the table/column, and can be reused on any dataset. DQOps comes with dozens of built-in sensors covering all common data quality metrics (and you can define custom ones). Sensors are analogous to “metrics” in Great Expectations or “measurements” in Deequ. They do not decide if something is good or bad; they just report a value from the data source ([How Data Quality Results are Stored? Examples and Best Practices](https://dqops.com/docs/dqo-concepts/data-storage-of-data-quality-results/#:~:text=When%20a%20data%20quality%20check,the%20%2060%20Parquet%20table)) ([How Data Quality Results are Stored? Examples and Best Practices](https://dqops.com/docs/dqo-concepts/data-storage-of-data-quality-results/#:~:text=The%20sensor%20readouts%20are%20processed,are%20added)). For example, the `daily_nulls_percent` sensor for a given column might return `actual_value = 12.5%` for today. Sensors also typically record context columns like `time_period` (e.g. the date) and identifiers for the data stream or partition if grouping is used ([How Data Quality Results are Stored? Examples and Best Practices](https://dqops.com/docs/dqo-concepts/data-storage-of-data-quality-results/#:~:text=DATE_TRUNC,time_period_utc%20ORDER%20BY%20time_period%2C%20time_period_utc)) ([How Data Quality Results are Stored? Examples and Best Practices](https://dqops.com/docs/dqo-concepts/data-storage-of-data-quality-results/#:~:text=2,the%20beginning%20of%20the%20month)). These results get stored as **sensor readouts** over time.

*Configuration:* Sensors are mostly predefined (in the DQOps “home” dictionary of built-ins), but if you create a custom sensor, you’d add a YAML file under `sensors/` (with a `.dqosensor.yaml` extension) describing the SQL template and any parameters ([DQOps YAML files reference](https://dqops.com/docs/reference/yaml/#:~:text=Sensor%20definition%20%20sensors%2F,dqocheck.yaml%20The%20definition%20of%20a)). You might do this if you have a very domain-specific metric to capture. There are also provider-specific sensors (in `...<provider>.dqoprovidersensor.yaml`) to override how a sensor works on a specific database dialect if needed ([DQOps YAML files reference](https://dqops.com/docs/reference/yaml/#:~:text=data%20quality%20sensor%2C%20with%20the,quality%20sensor%20and%20a%20data)). End users rarely need to edit sensor definitions – they mostly enable/disable existing sensors via checks.

**Rules (Data Quality Rules):** A rule evaluates the output of a sensor to determine if it meets expectations. It’s essentially the *condition or threshold*. Rules in DQOps are defined by a YAML spec (parameters like what mode it operates in, default bounds, etc.) and a Python function that implements the logic ([DQOps User Home Folder](https://dqops.com/docs/dqo-concepts/dqops-user-home-folder/#:~:text=DQOps%20User%20Home%20Folder%20The,must%20have%20a%20evaluate_rule%20function)). For example, a simple rule might be “fail if actual_value > X”. A more complex rule could be “compute a z-score from the actual_value and recent history, fail if z-score > 3”. DQOps categorizes rules by type of evaluation: some are **static thresholds** (no history needed, just check against a fixed number), others are **historic anomalies or changes** (need previous values) ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=5,historical%20values%20of%20sensor%20readouts)). In the rule YAML (`.dqorule.yaml`), it can specify a `mode` like `previous_readouts` to indicate it uses historical data ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=historical%20values%20of%20sensor%20readouts)), and a `time_window` (how many past periods to consider) ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=%2A%20mode%20,evaluates%20the%20current%20sensor%20readout)). The actual Python logic is usually a function `evaluate_rule(actual: float, history: list[float]) -> bool` that returns True/False or a numeric score. DQOps’s built-in rules cover things like “max allowed value”, “min % of rows that must be non-null”, “stddev-based anomaly detection”, “percentage change threshold”, etc.

*Configuration:* Users typically don’t write rules from scratch unless needed. You *activate* or configure a rule by setting its parameters in a check’s config. For instance, if using a threshold rule, you’d specify the threshold value for warning/error/fatal in the check YAML. If needed, one can create custom rule definitions under `rules/` with a custom Python module (the platform looks for a matching Python file alongside the `.dqorule.yaml`). By separating rules, DQOps allows one sensor’s output to be assessed in different ways. The rule definitions also indicate how severity levels map – e.g. a rule can define separate cutoffs for warning vs error severity.

**Checks:** A data quality *check* is the union of a sensor and a rule, applied to a particular target (table/column) and configured with specific parameters. In other words, a check says *“run this sensor on that data, and then apply this rule to decide pass/fail.”* For example, *“Daily Nulls Percent Anomaly on `customers.email`”* is a check: it uses the null percent sensor on the `customers.email` column, and uses an anomaly detection rule (with perhaps a 1% significance for flagging anomalies) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=according%20to%20default%20Table%20and,level%20data%20quality%20policies)) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=Target%20Check%20name%20Description%20table,count%20is%20greater%20than%2010)). Checks can be thought of as test cases or monitoring metrics. They are what you enable or disable in your table configurations.

**Targets and Hierarchy:** Checks are defined at different granularities:

- A *table-level check* targets a whole table (often a metric like row count, or schema checks).
- A *column-level check* targets one column (like validity or completeness of that column).
- A *partition check* targets each partition of a table (the check runs separately per partition key, e.g. per day or per country, providing partition-level outcomes) ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=)).
- A *table comparison check* compares two tables (source vs target), often by running a sensor on both and comparing results ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=)).

DQOps organizes checks in categories, mainly **Profiling**, **Monitoring (Observability)**, and **Partitioned** checks. *Profiling checks* are typically run infrequently (monthly) to profile data (they might not raise alerts but provide baseline metrics). *Monitoring checks* run regularly (daily or more) on aggregate table/column data to catch issues in the whole dataset. *Partitioned checks* run on slices of data (e.g., each day’s data separately) to pinpoint localized issues. This categorization is reflected in the YAML structure: each table YAML has sections for `monitoring_checks`, `profiling_checks`, and `partitioned_checks` (and further divided into dimensions like volume, schema, accuracy, etc.) ([DQOps YAML file definitions](https://dqops.com/docs/reference/yaml/TableYaml/#:~:text=TableMonitoringCheckCategoriesSpec%20partitioned_checks%20Configuration%20of%20table,The%20table%20does%20not)). For instance, `TableMonitoringCheckCategoriesSpec` in the YAML might contain a `volume` category which then lists `daily_row_count` check settings ([DQOps YAML file definitions](https://dqops.com/docs/reference/yaml/TableYaml/#:~:text=TableMonitoringCheckCategoriesSpec)).

**YAML Configuration Model:** All these abstractions are configured via YAML files in the DQOps user home (unless done through the UI, which ultimately writes YAML). Key file types include ([DQOps YAML files reference](https://dqops.com/docs/reference/yaml/#:~:text=Type%20File%20location%20Description%20Data,section%20of%20the%20user%20interface)) ([DQOps YAML files reference](https://dqops.com/docs/reference/yaml/#:~:text=Default%20notification%20webhooks%20%20settings%2Fdefaultnotifications,activated%20on%20all%20imported%20tables)):

- **Connection YAML (`*.dqoconnection.yaml`):** Stores connection info (JDBC URL, etc.) and default settings for that data source. It also contains the schedule (CRON expressions) for that connection’s checks ([DQOps YAML files reference](https://dqops.com/docs/reference/yaml/#:~:text=Type%20File%20location%20Description%20Data,section%20of%20the%20user%20interface)). You might also define default incident grouping or notifications here for that source.
- **Table YAML (`*.dqotable.yaml`):** Stores metadata about a specific table and which checks are enabled on it. The `TableSpec` inside includes properties like `disabled` flag, `priority` (business importance of the table), groupings, and then sections for checks ([DQOps YAML file definitions](https://dqops.com/docs/reference/yaml/TableYaml/#:~:text=Property%C2%A0name%C2%A0%20%C2%A0Description%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%20%C2%A0Data%C2%A0type%C2%A0%20%C2%A0Enum%C2%A0values%20Default%C2%A0value%C2%A0,quality%20issues%20on%20less%20important)) ([DQOps YAML file definitions](https://dqops.com/docs/reference/yaml/TableYaml/#:~:text=,alias%7D%20to%20replace%20the)). For example, under `monitoring_checks`, you’d have categories (volume, schema, etc.), and within those, each specific check with its rule parameters. Enabling or disabling a check is as easy as adding or removing its YAML node (or setting an `enabled: false`). The table YAML is essentially the single source of truth for all checks active on that table. It also can store per-column check configs under a `columns:` map (for column-level checks).
- **Data Quality Policies (`*.dqopattern.yaml`):** Instead of enabling checks table by table, DQOps supports *pattern-based policies*. A table-level policy (and similarly a column-level policy) can define a set of checks that should apply to all tables that match a pattern ([DQOps YAML files reference](https://dqops.com/docs/reference/yaml/#:~:text=default%20webhooks%20used%20for%20data,activated%20on%20all%20imported%20tables)). The “default Table and Column-level data quality policies” mentioned in the docs ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=Once%20new%20tables%20are%20imported%2C,m)) are such YAML files that say, for example, “enable `daily_row_count` and `daily_nulls_percent` on all tables by default.” These policies are applied upon metadata import to auto-populate each new table’s checks, which is why after importing a table, you already have some default checks activated without manual effort. This encourages a standardized baseline of monitoring across all datasets.
- **Custom Definitions:** As noted, if you need to extend DQOps:
  - *Sensor definitions* (custom sensor SQL) go in `sensors/` (with optional provider-specific subfolder) ([DQOps YAML files reference](https://dqops.com/docs/reference/yaml/#:~:text=Sensor%20definition%20%20sensors%2F,dqocheck.yaml%20The%20definition%20of%20a)).
  - *Rule definitions* (custom rule logic) go in `rules/` ([DQOps YAML files reference](https://dqops.com/docs/reference/yaml/#:~:text=sensors%2F,up%20a%20data%20quality%20check)).
  - *Check definitions* (as templates pairing a sensor and rule to form a new kind of check) go in `checks/` ([DQOps YAML files reference](https://dqops.com/docs/reference/yaml/#:~:text=Data%20quality%20rule%20definition%20,up%20a%20data%20quality%20check)). This is advanced usage – for example, if you create a brand new metric and rule, you’d also register a check definition so it appears as a named check type in the UI.
  - These custom files in user home will **override** built-ins if they have the same name, due to how DQOps loads its configurations (user definitions take precedence, allowing you to modify built-in check behavior if needed) ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=the%20metric%2C%20raising%20a%20data,issue%20if%20the%20rule%20fails)) ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=The%20definitions%20of%20custom%20checks%2C,the%20custom%20definition%20is%20used)).

**Metadata Tracked Across Runs:** DQOps keeps extensive metadata to support its features:

- **Connection & Table Metadata:** The YAML files themselves are metadata – storing connection details, schema info (list of columns, data types), check configurations, labels, comments, etc. For example, you might label a table with `stage: "Staging"` or assign an `owner` in the YAML for documentation purposes ([DQOps YAML file definitions](https://dqops.com/docs/reference/yaml/TableYaml/#:~:text=,integer)).
- **Basic Statistics:** As part of profiling, DQOps stores statistics (like distinct counts, min/max, etc.) for each column in the `statistics` parquet, keyed by table and column ([How Data Quality Results are Stored? Examples and Best Practices](https://dqops.com/docs/dqo-concepts/data-storage-of-data-quality-results/#:~:text=5,intended%20to%20be%20modified%20manually)) ([How Data Quality Results are Stored? Examples and Best Practices](https://dqops.com/docs/dqo-concepts/data-storage-of-data-quality-results/#:~:text=6,intended%20to%20be%20modified%20manually)). This provides a profile of the data content.
- **Sensor Readouts:** Every time a check runs, the metric value is saved in `sensor_readouts` with a timestamp, connection/table/column identifiers, etc. ([How Data Quality Results are Stored? Examples and Best Practices](https://dqops.com/docs/dqo-concepts/data-storage-of-data-quality-results/#:~:text=4,This)) ([How Data Quality Results are Stored? Examples and Best Practices](https://dqops.com/docs/dqo-concepts/data-storage-of-data-quality-results/#:~:text=5,intended%20to%20be%20modified%20manually)). This is essentially time-series data for each check’s metric.
- **Check Results:** After rule evaluation, the outcome (pass or fail and at what severity) is stored in `check_results` with references to the sensor reading and the check definition ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=The%20check%20results%20after%20validation,the%20partition%20day%20or%20month)). It also includes the severity level (warning/error/fatal) for failed checks ([How Data Quality Results are Stored? Examples and Best Practices](https://dqops.com/docs/dqo-concepts/data-storage-of-data-quality-results/#:~:text=The%20sensor%20readouts%20are%20processed,are%20added)) ([How Data Quality Results are Stored? Examples and Best Practices](https://dqops.com/docs/dqo-concepts/data-storage-of-data-quality-results/#:~:text=check_results%20table%2C%20but%20additional%20columns,are%20added)).
- **Errors:** Any errors encountered during execution (e.g., SQL execution errors, template rendering errors, rule exceptions) are logged in the `errors` table with details and stack traces ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=Using%20untested%20custom%20sensors%20and,which%20supports%20only%20numeric%20columns)) ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=4.%20,to%20the%20errors%20parquet%20table)). This is important for troubleshooting misconfigurations or connector issues.
- **Error Samples:** For certain checks that identify “bad” rows (like a value that doesn’t match regex or is out of range), DQOps can collect sample failing rows. These are stored in `error_samples` parquet. For instance, if a column validity check fails 100 rows, it might store a sample of up to N of those rows for inspection. This helps in debugging data issues by seeing actual offending values.
- **Incidents:** As described, incident records track ongoing issues. An incident entry will have fields like status, issue hash (to group occurrences), first seen, last seen, count of occurrences, etc. ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=Grouping%20data%20quality%20issues%20into,until%20the%20issue%20is%20resolved)). When an incident is resolved or muted by the user, that status is updated in this metadata as well.
- **Audit and Execution Logs:** The system also records when jobs ran, how long they took, etc., possibly in `.logs` or an index. The `.index` folder mentioned in docs ([How Data Quality Results are Stored? Examples and Best Practices](https://dqops.com/docs/dqo-concepts/data-storage-of-data-quality-results/#:~:text=6,intended%20to%20be%20modified%20manually)) ([How Data Quality Results are Stored? Examples and Best Practices](https://dqops.com/docs/dqo-concepts/data-storage-of-data-quality-results/#:~:text=7.%20The%20,intended%20to%20be%20modified%20manually)) tracks sync state with the cloud (to know which local files have been pushed remotely).

All of this metadata is persisted across runs, meaning DQOps builds a knowledge base of your data’s quality history. The benefit is that it can do things like *incremental checks* – by knowing what the last checked date was and storing a time window, DQOps can limit analysis to new data only ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=DQOps%20supports%20incremental%20data%20quality,on%20the%20monitored%20data%20source)). For example, if a table is huge, you might only want to check the last 7 days of data each run; DQOps can use the stored state to only query the new partition range (this is configured via `incremental_time_window` in the table spec if needed ([DQOps YAML file definitions](https://dqops.com/docs/reference/yaml/TableYaml/#:~:text=data%20quality%20checks%20,a%20GROUP%20BY%20clause%2C%20calculating)) ([DQOps YAML file definitions](https://dqops.com/docs/reference/yaml/TableYaml/#:~:text=incremental_time_window%20%20Configuration%20of%20the,by%20the%20data%20grouping%20name))).

In essence, DQOps’s configuration model treats data quality rules as code (YAML code), and the system’s state is mostly these files plus the accumulated results in Parquet. This model makes it easy to migrate or copy – e.g., copying the user home folder copies your entire configuration and history. It also means standard DevOps processes (like code review for YAML changes, CI to lint them, etc.) can be applied. The abstraction of sensors/checks/rules might seem complex at first, but the UI shields casual users from it by presenting “checks” as the main concept. Under the hood, understanding sensors and rules is powerful for customization but not mandatory for basic use.

## Getting Started Workflow Example

Let’s walk through a typical DQOps workflow for a new user (a seasoned data engineer) using the official Getting Started guide steps:

### **Step 1: Installation and Launch**

First, you need to install DQOps. Suppose you choose the simple route: using Python. You would run `pip install dqops` and then launch the app with `python -m dqops`. This will download the DQOps engine (if not already) and start the web UI on `http://localhost:8888` by default ([DQOps Data Quality Engine Architecture](https://dqops.com/docs/dqo-concepts/architecture/dqops-architecture/#:~:text=The%20simplest%20way%20to%20start,as%20a%20dqops%20PyPi%20package)). You’ll see logs indicating the web server is up. You can also run DQOps via Docker (`docker run dqops/dqo`...) or as a JAR if you prefer. Once it’s running, open your browser to the localhost address to access the UI.

### **Step 2: Connect to a Data Source**

In the DQOps UI, navigate to the *Data Sources* section. Click “Add Connection” (or “Connect to a data source”). You will be prompted to enter connection details. For example, if you have a CSV file of data (as in the tutorial example), you might select *CSV* as the provider, give the connection a name (say “LocalCSV”), and point it to the file path of your CSV ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=,14)) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=After%20installation%20and%20starting%20DQOps%2C,monitoring%20and%20run%20basic%20statistics)). (If it were a database like PostgreSQL, you’d enter host, username, etc.) After saving, the new connection appears in the left tree panel. At this point, no tables are imported yet.

Now, import the table metadata. Click on the connection name, go to the *Schemas* or *Import* tab, and DQOps will list available tables (for CSV, it treats the file as a table). Select the table (e.g., `orders.csv`) and import it ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=,for%20columns%20in%20this%20connection)). DQOps will create the table entry under the connection in the tree. You have now a `orders.csv` table with its columns recognized.

### **Step 3: Run Initial Profiling (Basic Statistics & Profiling Checks)**

With the table imported, you can now profile it. Ensure the table is selected, then find the *Start Profiling* button (often on the top right in the UI). Click it to begin the data assessment job ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=Upon%20import%2C%20you%20will%20receive,button%20to%20initiate%20this%20process)). What this does: it triggers the collection of basic stats and executes the default **profiling checks** that DQOps auto-activated according to its default policy ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=Once%20new%20tables%20are%20imported%2C,m)). In our `orders.csv` example, DQOps might run checks like:

- *Table-level:* row count (just to capture how many rows),
- *Column-level:* checks for any columns containing emails or phone numbers, etc. ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=Target%20Check%20name%20Description%20column,Detects%20emails%20inside%20text%20columns)) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=column%20profile%20contains%20usa%20phone,Detects%20emails%20inside%20text%20columns)).

These default checks are designed to uncover any obvious data issues or patterns without configuration. In addition, DQOps will compute summary statistics on each column (like min, max, avg, null count). The job might take a short time depending on data size (since it’s querying/reading the data). Once done, you’ll get a notification (in the app UI) that profiling completed.

### **Step 4: Review Profiling Results**

Now, still in the UI, you can review what happened. Navigate to the *Profiling* results for `orders.csv`. There will be a tabular view of **Basic Data Statistics** ([Review results from data quality assessment and automatically configure data quality checks](https://dqops.com/docs/getting-started/review-results-and-run-monitoring-checks/#:~:text=1,the%20Basic%20data%20statistics%20tab)). For example, you might see each column of `orders.csv` listed with statistics: distinct count, null count, top values, etc. If the dataset is small, the values show immediately; if large, some might be approximate.

Next, check the **Profiling Checks** results. There is likely a “Table quality status” indicating how many profiling checks passed. Click the *Profiling* -> *Checks* tab to see individual checks. Suppose `orders.csv` has a column `email`. The “profile contains email percent” check might show, say, 98% (meaning 98% of rows have something that looks like an email) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=Target%20Check%20name%20Description%20column,Detects%20emails%20inside%20text%20columns)) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=column%20profile%20contains%20usa%20phone,Detects%20emails%20inside%20text%20columns)). This isn’t necessarily a failure – it’s just reporting a metric because the rule might not be set to flag anything (it’s more an informational profiling check). If a profiling check did have a threshold, it would show if it passed or failed. Often, profiling checks are set to *not* fail but just record metrics. So likely everything is “green” at this point, unless the default policy included something like a “null percent anomaly” and your data had a weird distribution even within the initial run.

You can also click on a column in the left tree (e.g., a specific column under the table) and view column details. DQOps would show distribution of values, etc., which helps you understand the data. At this stage, you are essentially doing exploratory data profiling using DQOps’s interface.

### **Step 5: Configure Monitoring Checks (Rule Mining)**

Now that you know what the data looks like, you want to set up ongoing data quality checks – for daily monitoring. Instead of manually adding checks one by one, use DQOps’s **Rule Mining** feature to auto-generate them. In the UI, still under the table, find the *Data Quality Rule Mining* tab (as instructed in the tutorial) ([Review results from data quality assessment and automatically configure data quality checks](https://dqops.com/docs/getting-started/review-results-and-run-monitoring-checks/#:~:text=To%20navigate%20to%20the%20data,quality%20rule%20miner)). Once you open it, DQOps immediately analyzes the stats and any data samples and proposes a set of checks ([Review results from data quality assessment and automatically configure data quality checks](https://dqops.com/docs/getting-started/review-results-and-run-monitoring-checks/#:~:text=The%20rule%20mining%20screen%20allows,the%20proposed%20check%20configuration)). You’ll see a list populate, maybe showing sections like “Table checks” and “Column checks.” For example:

- It might propose a “daily row count change” check for `orders.csv` with a 10% threshold (to catch if row count changes by more than 10% day-over-day) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=identifies%20empty%20tables,The%20table%20must%20be)) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=volume,a%20table%20exists%2C%20can%20be)).
- For a numeric column (say `order_amount`), it might propose a “daily avg anomaly” check if that’s relevant, or a validity check if there are known patterns.
- For a text column like `email`, since 98% looked like emails, it might propose a check “valid email format” that would fail if that percentage drops or if it finds invalid emails beyond a tolerance.
- It will also suggest **baseline checks** that currently pass, which serve as drift detectors. For instance, if `country` column currently has 5 distinct values, it might suggest a check to ensure the set of values doesn’t change unexpectedly (i.e., a new country code appears could be flagged).

You can refine the suggestions. Let’s say you only care about certain types of checks; you can filter by category or adjust the “Error rate (%)” slider to make it more or less sensitive ([Review results from data quality assessment and automatically configure data quality checks](https://dqops.com/docs/getting-started/review-results-and-run-monitoring-checks/#:~:text=You%20can%20also%20customize%20the,proposals%20with%20the%20following%20options)) ([Review results from data quality assessment and automatically configure data quality checks](https://dqops.com/docs/getting-started/review-results-and-run-monitoring-checks/#:~:text=,quality%20checks%20to%20detect%20data)). By default, 2% error rate is allowed, meaning it tries to allow up to 2% of bad data before failing ([Review results from data quality assessment and automatically configure data quality checks](https://dqops.com/docs/getting-started/review-results-and-run-monitoring-checks/#:~:text=,quality%20checks%20to%20detect%20data)). If you lower that, checks become stricter.

Review each proposed check. For example, “daily nulls percent anomaly on `address` column” – maybe your `address` column had 0% nulls in profiling; the rule miner might propose to flag it if nulls exceed, say, 5%. If that sounds reasonable, keep it. If you know `address` can be null sometimes and you don’t care, you could remove that proposal.

Once satisfied, click **Apply**. DQOps will then convert those proposals into actual configuration. Behind the scenes, it updates the `orders.csv.dqotable.yaml` file, adding entries under the monitoring checks for each selected check with the rule thresholds determined. Now your table is configured with a set of monitoring checks ready to run daily.

### **Step 6: Running Monitoring Checks and Interpreting Results**

With monitoring checks in place, you can either wait for the next scheduled run (say tonight at 12:00) or run them immediately to test. Let’s run them now. In the UI, go to the *Monitoring* section for the table and click “Run checks” (or use the CLI `check run`). This triggers the DQOps engine to execute all daily checks for `orders.csv`. It will likely run the combined sensor query (which might include multiple metrics). When complete, refresh or navigate to the *Monitoring Checks* results for the table.

Now you will see a **Monitoring Table Quality** status. Perhaps everything passed (if the data is consistent with the profile). But for illustration, suppose one check failed: say the “daily distinct count anomaly” for `country` column flagged an issue ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=column%20daily%20empty%20column%20found,For%20example%2C%20when%20a%20column)) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=column%20daily%20distinct%20count%20anomaly,a%20text%20value%20was%20found)). On investigating, you see that yesterday there were 5 distinct country values, but today there are 6 – a new country code appeared that wasn’t in the historical data. The check was configured to alert on any significant change in distinct count, so it raised a warning. This might indicate new data from a region we haven’t seen before, which could be fine or could be unexpected (maybe someone started entering “Antarctica” as a country – trivial example). DQOps would mark that check in *warning* state for today.

Another example: maybe the “daily row count” check is fine (table isn’t empty), but the “row count anomaly” check is warning that today’s row count is 30% lower than yesterday – that’s above the 10% threshold we set ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=identifies%20empty%20tables,The%20table%20must%20be)) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=volume,a%20table%20exists%2C%20can%20be)). That could indicate a potential upstream issue (perhaps a partial load). DQOps would highlight this with an *error* or *warning* (depending on how the rule was set, maybe top 1% changes count as errors) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=Target%20Check%20name%20Description%20table,count%20is%20greater%20than%2010)) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=identifies%20empty%20tables,count%20is%20greater%20than%2010)).

In the UI, you can click on each failed check to see details. For an anomaly, it might show a small sparkline of past values and indicate why today’s value is out of expected range. DQOps also allows you to view **sample data for errors** if configured – for example, if a regex check for emails failed, you could see some of the values that didn’t match (maybe “N/A” or malformed strings).

### **Step 7: Incident Management**

Because some checks failed, DQOps will have created incidents. Let’s say the distinct country anomaly is minor, but the row count drop is major. The *Incidents* tab for the connection or table will list something like “Row count anomaly detected on orders.csv” as a new incident. It will have a severity (perhaps Warning or Error) and an open status ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=incident%20%20that%20can%20be,until%20the%20issue%20is%20resolved)). As a data engineer, you investigate this incident. Maybe you confirm there was a truncated load file. You decide to notify the pipeline team. You can use DQOps to send a notification: if Slack webhooks were set up, it might have already posted one. If not, you could copy the incident details and paste into a Slack manually or mark it as acknowledged in DQOps so others know it’s being looked at.

After fixing the data (say you reload the missing data), the next run of checks should pass, and DQOps will automatically mark the incident as **resolved** (if the issue no longer occurs for a configured number of days) or you can manually mark it resolved once you’re satisfied ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=DQOps%20groups%20similar%20data%20quality,until%20the%20issue%20is%20resolved)). If an incident recurs, DQOps will reopen or update it, incrementing counts.

DQOps’s incident view provides a timeline of when the issue was seen and resolved. This helps in post-mortems – you can track how long data issues persisted and how often they happen.

### **Step 8: Dashboarding and Ongoing Monitoring**

Finally, you want to share the overall data quality metrics with stakeholders. Ensure your DQOps instance is paired with a cloud account (the UI has a place to enter a pairing key for DQOps Cloud in Local Settings). Then click *Synchronize* to push the results to cloud ([Review data quality results on data quality dashboards](https://dqops.com/docs/getting-started/review-results-on-dashboards/#:~:text=To%20display%20results%20on%20data,corner%20of%20the%20navigation%20bar)). Next, go to the *Data Quality Dashboards* section in the UI. Choose, for instance, the *Current Data Quality KPI* dashboard. Here you’ll see a card for `orders.csv` showing maybe a 95% success rate (if out of 20 checks, one was in warning = slight deduction). The *Current Column Status* dashboard might show a bar chart per column highlighting that `country` had 1 issue (the anomaly) ([Review data quality results on data quality dashboards](https://dqops.com/docs/getting-started/review-results-on-dashboards/#:~:text=,Use%20cases%20and%20examples)). Over time, as you accumulate daily results, the *Trend* dashboards will show lines for each check’s value (like row count over time, null % over time) and highlight points where incidents occurred.

This whole workflow demonstrates how a data engineer can get from zero to a fully monitored data asset: DQOps handled connecting to the data, understanding it via profiling, suggesting what to monitor, and now keeps an eye on it daily – alerting the team when something goes off track. The engineer can refine checks further (maybe add a new custom check if something specific comes up, or adjust thresholds to reduce false alarms). They can also onboard more tables by repeating the import and rule mining process, scaling out the monitoring to dozens or hundreds of tables systematically.

Throughout the process, all changes (connection setup, check configurations) are saved in YAML, so one could export or commit these to a repository. DQOps’s CLI could also accomplish many steps: for example, one could run `dqo connection create` (if such a command exists) to script adding a connection, use `dqo metadata import` to pull schemas, etc., enabling a code-driven approach. But the UI makes the onboarding interactive and quick.

## Design Decisions and Limitations

DQOps’s design makes certain assumptions and trade-offs to balance flexibility, ease of use, and performance:

- **YAML & GitOps vs Database Config:** The choice to store all configuration in YAML files is a conscious design for transparency and version control ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=DQOps%20does%20not%20use%20a,visible%20in%20the%20user%20interface)). The upside is that configurations are human-readable, easy to templatize, and track in Git (each change can be reviewed). This aligns with DevOps practices (Infrastructure as Code, or in this case Data Quality as Code). A trade-off is that extremely large numbers of checks might be harder to manage as flat files and could benefit from a database. However, DQOps mitigates this with hierarchical organization (one file per table, etc.) and by providing a UI to abstract the file edits. Another implication is multi-user editing: if two engineers edit the same YAML, you need Git to merge changes. DQOps addresses collaborative editing by funneling changes through the UI and (in cloud scenarios) perhaps locking, but in pure file form, coordination is left to standard practices (lock files or Git discipline).

- **Java Engine with Python Extensibility:** DQOps being built in Java (with Spring) but installable via Python is an interesting design. The Java choice likely brings performance (JVM optimizations, robust JDBC connectivity, multi-threading), which is beneficial for running heavy queries and parallel checks. The Python client and rule scripts, however, give it flexibility – users can write custom rules in Python (familiar to data scientists) rather than needing to write Java code ([DQOps User Home Folder](https://dqops.com/docs/dqo-concepts/dqops-user-home-folder/#:~:text=DQOps%20User%20Home%20Folder%20The,must%20have%20a%20evaluate_rule%20function)). This hybrid approach means the system is not fully “in-python” like Great Expectations, but for typical usage you don’t need to know it’s Java under the hood. The installation process of fetching a JRE might be a minor inconvenience but it’s automated. One limitation here: because the core is Java, deeply customizing internal logic beyond sensors and rules is not trivial (the source is open, but not Python). That said, the provided extension points (sensors SQL and rules in Python) cover most needs without touching core code.

- **Embedded Scheduler vs External Orchestration:** DQOps includes its own scheduler (Cron-based per connection) ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=calling%20the%20run%20checks%20operation)), which is great for simplicity – you don’t have to set up Airflow or cron jobs just to run daily checks. However, this internal scheduler is not as sophisticated as enterprise schedulers: for instance, it doesn’t have dependency management or complex conditional scheduling. It’s basically “run these checks at noon daily”. If your data load completes at variable times, you might still want to trigger DQOps via an external orchestrator at the right moment (using API/CLI). The design seems to assume either the schedule will be aligned to daily patterns or that external triggers will be used for more dynamic workflows. This is a reasonable trade-off: include a basic scheduler for general use, but allow integration with tools like Airflow for advanced cases ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=Configure%20data%20quality%20checks%20in,code%2C%20with%20code%20completion)). In an enterprise with existing pipelines, one might disable the internal scheduler and use the Python client in their ETL jobs to call DQOps after data ingestion. The flexibility is there, but it requires understanding of both options.

- **Data Storage and Volume:** Storing results as Parquet in the local file system makes DQOps easy to deploy (no need to manage a separate database for results). And Parquet is efficient for large volumes of time-series data. But as a limitation, analyzing those results outside DQOps requires either using the DQOps API or reading Parquet files with another tool. Some users might prefer results in a SQL database where they can join with other data. DQOps addresses this by providing the cloud sync to BigQuery – essentially offering a data warehouse of results for analysis ([DQOps Data Quality Engine Architecture](https://dqops.com/docs/dqo-concepts/architecture/dqops-architecture/#:~:text=,using%20a%20dedicated%20DQOps%20Looker)). If an organization doesn’t want to use DQOps Cloud and still wants a central DB for results, they’d have to build a custom solution (perhaps an export job or mount the files to a query engine). Another minor limitation: the `.data` folder could grow large if you monitor many tables over years. You’ll need a strategy for archiving or pruning old data if that becomes an issue (though Parquet compresses well, and you can configure retention by limiting time windows for incremental checks ([DQOps YAML file definitions](https://dqops.com/docs/reference/yaml/TableYaml/#:~:text=data%20quality%20checks%20,a%20GROUP%20BY%20clause%2C%20calculating)) ([DQOps YAML file definitions](https://dqops.com/docs/reference/yaml/TableYaml/#:~:text=incremental_time_window%20%20Configuration%20of%20the,by%20the%20data%20grouping%20name))).

- **Merging Queries – Complexity vs Performance:** A notable design decision is the automatic merging of sensor queries ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=connectors%2C%20such%20as%20Google%20BigQuery,Java%20native%20libraries)). This improves performance by doing one full table scan to get multiple metrics, but it introduces complexity in error handling. As documented, if one of the merged sub-query components has an error (say a sensor that is not compatible with the database), the whole merged query fails, and DQOps then has to retry by splitting queries ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=Error%20management%20for%20merged%20SQL,by%20the%20monitored%20data%20source)). This design assumes that the benefit of fewer scans outweighs the cost of this extra logic, which is likely true for large tables. For most users this is transparent – except maybe when debugging why a check didn’t run, you might find an error in the logs about merged query failure and subsequent partial execution. It’s a clever optimization not found in simpler tools, but the trade-off is complexity under the hood. So far, the platform seems to manage it gracefully by catching errors and re-running as needed.

- **Thresholds and False Positives:** DQOps provides default thresholds for anomaly detection (like “top 1% biggest change” or “>3σ from mean”) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=identifies%20empty%20tables,The%20table%20must%20be)) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=volume,a%20table%20exists%2C%20can%20be)). These defaults might not fit all cases, so the platform allows adjusting them. There’s an inherent challenge in any automated data quality system: avoiding alert fatigue vs missing real issues. DQOps’s rule mining and multi-level severity are attempts to balance this. However, a limitation is that an auto-configured rule might still produce false positives if the data inherently fluctuates. It requires the user to review and possibly tweak thresholds or mute certain checks. This is not a flaw per se, but an area where human oversight is needed. The design decision to include severity levels is wise – it means you can set, say, a very sensitive threshold at warning (so you keep an eye on it) but only alert if it crosses a higher error threshold.

- **UI vs Code parity:** DQOps’s UI is powerful, but some advanced configurations (like adding a custom sensor or rule code) require file edits or using the CLI. The design tries to cover most routine tasks in UI (adding connections, enabling checks, setting schedules, viewing results) and it largely succeeds. One limitation might be that editing YAML directly (for power users) and using the UI concurrently could lead to confusion if not synced. But DQOps seems to reload config from disk on changes. Also, not everything may be exposed in UI (for example, the UI has a check editor but perhaps not an interface to write a brand new sensor’s SQL – that you’d do in code). As the platform evolves, more of these might be surfaced.

- **Multi-tenancy and Security:** In an enterprise, you might have multiple teams or departments using one DQOps instance. The current open-source DQOps does not mention granular role-based access control (RBAC). It seems to assume a trusted environment or a single team. If you needed to restrict access (e.g., Team A shouldn’t see Team B’s data quality results), you’d have to run separate instances or wait for a future feature. The cloud service likely has user account separation (each user gets their own space), but on a shared on-prem instance, RBAC is a potential limitation at the moment. This is a typical trade-off for a young open-source project (features like RBAC often come later or in enterprise editions).

- **Under Development / Roadmap Areas:** While not explicitly in the docs we cited, one can infer that DQOps is actively adding connectors and expanding its check library. As of late 2024, it supports many sources (SQL, NoSQL, file-based, etc.) ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=Supported%20data%20sources)). Any sources not supported could be a limitation (though you can often use generic JDBC). Features like data grouping up to 9 levels ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=)) show they are thinking about complex use cases (like multi-dimensional data segmentation). Some things that might be on the roadmap could include: a more sophisticated machine learning approach to anomaly detection (beyond the current rule formulas), better collaboration tools (like assignments and comments on incidents), and perhaps more cloud integration options (like sending metrics to Splunk or DataDog for observability convergence). The documentation already references “data quality KPIs” and improvement processes ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=DQOps%20measures%20data%20quality%20with,supports%20custom%20data%20quality%20dashboards)), which suggests a vision beyond just monitoring – possibly tying into data governance goals.

In summary, DQOps’s design heavily emphasizes **scalability, automation, and integration** – it assumes you want to monitor potentially *very large tables* (hence incremental checks and query merging) ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=DQOps%20supports%20incremental%20data%20quality,on%20the%20monitored%20data%20source)), you want to automate threshold setting (rule mining), and you want to integrate with DevOps (YAML config, API) ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=DQOps%20does%20not%20use%20a,visible%20in%20the%20user%20interface)). The flipside of these choices is that users need to invest a bit in understanding the YAML structure and might occasionally troubleshoot a Jinja template or a JDBC driver issue. However, these are common trade-offs in a flexible system. The provided defaults and UI should cover most needs, and the architecture is robust enough (with job queue, error handling, etc.) to run in production reliably. Limitations like lack of fine-grained access control or the need to use the cloud for fancy dashboards are areas to be aware of, but not blockers for most teams adopting it.

## Enterprise-Oriented Capabilities

DQOps includes several features that appeal to enterprise data engineering teams, aiming to fit into production environments and governance processes:

- **Built-in Scheduling:** DQOps can operate as a lights-out monitoring service thanks to its internal scheduler. You can configure CRON expressions per connection (data source) to run checks at desired times ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=anomalies%2C%20empty%20tables%2C%20table%20availability%2C,m)) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=To%20review%20scheduling%20for%20profiling,and%20daily%20monitoring%20checks)). This means an enterprise can deploy DQOps on a server and trust it to continually audit data daily/weekly. No need for a separate scheduling system for basic schedules (though integration with enterprise schedulers is also supported if needed). The scheduling config is part of the connection YAML, which makes it easy to standardize (e.g., all prod data sources run dailies at midnight).

- **Alerting & Notification Integration:** Enterprises need prompt notification of issues. DQOps supports webhook-based alerts out-of-the-box ([How Data Quality Checks Are Executed - Data Flow](https://dqops.com/docs/dqo-concepts/architecture/data-quality-check-execution-flow/#:~:text=8,incidents)). A common use-case is Slack integration – by configuring a Slack webhook URL (either globally or per data source), any new incident will result in a JSON message posted to Slack (or Microsoft Teams, etc.). The payload contains incident details, which can be further integrated into ticketing systems (like create a JIRA ticket via a webhook listener). This means DQOps can seamlessly hook into existing incident management workflows. Additionally, the incidents can be viewed and managed within DQOps by multiple users, supporting a DevOps model of acknowledging and resolving issues.

- **Version Control and CI/CD:** All DQOps configurations being in YAML means teams can treat data quality rules as code. Enterprises can store these YAML files in a Git repository alongside source code or ETL code. They can implement code reviews for any changes to data quality rules (ensuring proper oversight on threshold changes, etc.). DQOps being **DevOps-friendly** also extends to automation: you can incorporate DQOps in CI pipelines. For example, you might have a CI job that runs `dqops check run` on a staging dataset whenever a new ETL pipeline is deployed, to run a battery of data quality tests as “unit tests” for the data ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=DQOps%20does%20not%20use%20a,visible%20in%20the%20user%20interface)). The Python client allows integration in a CI/CD pipeline script to, say, fail the pipeline if critical data quality checks fail. This can prevent bad data from being promoted to production. In short, DQOps can be part of continuous delivery of data pipelines, enforcing quality gates.

- **Integration with Orchestration Tools:** Enterprises often use orchestrators like **Apache Airflow**, **Apache NiFi**, or **dbt** for data workflows. DQOps provides ready integration points: it has Airflow operators to run checks as tasks ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=Configure%20data%20quality%20checks%20in,code%2C%20with%20code%20completion)), and it can be triggered in a dbt workflow (for instance, using the Python client in a dbt macro or script after model runs). They specifically mention integration guides for Airflow and dbt ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=Configure%20data%20quality%20checks%20in,code%2C%20with%20code%20completion)). This means DQOps can slot into existing data pipelines, ensuring that data quality checks are not siloed but part of the pipeline execution graph (e.g., “After loading table, run DQOps check task, on failure, notify and halt the downstream tasks”). This approach is essential for enterprise reliability, preventing bad data from propagating.

- **Multi-Environment Support:** Enterprises typically have dev, test, prod environments for their data (and thus for data quality configs). While DQOps doesn’t explicitly have an “environment” toggle, its design facilitates this separation. You can maintain separate user homes or separate projects for different environments. For example, you might have separate YAML repositories for dev and prod, or use branch per environment. The *stage* property on TableSpec ([DQOps YAML file definitions](https://dqops.com/docs/reference/yaml/TableYaml/#:~:text=,integer)) can be used to label data sets by environment or pipeline stage, which can reflect in filtering results or organizing checks. Additionally, DQOps Cloud’s pairing key can be set per instance, so a dev DQOps could sync to a dev cloud project, and prod to prod. The **portability of YAML** makes it straightforward to promote configurations from dev to prod: after testing checks in dev on sample data, you can copy the YAML to prod (or merge via Git) and then just reconnect it to the prod data source. Because the checks reference table/column names generically, if those exist in prod, it works the same. This manual promotion is analogous to how one might promote dbt projects between environments. There is no known hard limitation preventing running multiple DQOps instances for each environment if needed.

- **Scalability and Performance for Big Data:** Enterprise data can be huge. DQOps addresses this with features like **incremental checks** and **query merging** ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=DQOps%20supports%20incremental%20data%20quality,on%20the%20monitored%20data%20source)). Incremental mode allows you to only scan new partitions or the latest N days of data rather than the whole table each time (configured via `incremental_time_window` in YAML, defaulting to analyze recent data) ([DQOps YAML file definitions](https://dqops.com/docs/reference/yaml/TableYaml/#:~:text=data%20quality%20checks%20,a%20GROUP%20BY%20clause%2C%20calculating)). This is crucial for making daily checks feasible on large fact tables. Also, merging multiple checks into one SQL query reduces the overhead on warehouses (which can be costly or slow with too many queries). These optimizations show DQOps is built with enterprise-scale (both in data volume and number of checks) in mind. Furthermore, running as a Java service, it can handle parallelism and multi-threading, making full use of server resources to check many tables concurrently if needed.

- **Enterprise Connectors:** DQOps supports a wide range of data source types – relational databases (Oracle, SQL Server, MySQL, Postgres, etc.), cloud data warehouses (Snowflake ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=Image%3A%20MySQL%20%20%C2%A0%20%C2%A0,90)), BigQuery, Redshift ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=Image%3A%20Athena%20%20%C2%A0%20%C2%A0,90))), data lakes (Spark, Databricks ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=Image%3A%20Athena%20%20%C2%A0%20%C2%A0,90)), Hive, CSV/Parquet files ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=Image%3A%20Athena%20%20%C2%A0%20%C2%A0,90))), and even NoSQL/time-series stores (it mentions connectors like Athena, Trino, DuckDB, etc.) ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=services%2C%20table%20formats%2C%20and%20flat,files)). This breadth means an enterprise can use one tool (DQOps) to monitor across heterogeneous data platforms. For instance, you can have one DQOps instance checking data in Snowflake and also some CSV-based data lake, and even an Oracle OLTP, all together. That’s beneficial for centralizing data quality management across an organization’s data landscape. Each connection’s results can be aggregated into unified dashboards and incidents, giving a holistic view.

- **Reusable Templates & Inheritance:** Through the concept of *data quality policies* (table/column patterns) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=Once%20new%20tables%20are%20imported%2C,m)) ([Creating a connection to a data source and starting data quality assessment](https://dqops.com/docs/getting-started/add-data-source-connection/#:~:text=anomalies%2C%20empty%20tables%2C%20table%20availability%2C,m)), DQOps allows enterprises to define standard checks once and apply them everywhere. For example, an enterprise data governance team might decide that *every table* must have a row count check and a freshness check. They can encode this in a default table pattern YAML. Then whenever a new table is onboarded, those checks are automatically in place without manual configuration. This ensures consistency and saves time. It’s akin to having a global test suite that applies to all datasets, with the flexibility to override or disable per table if needed. This modular reuse is something that scales well when you have hundreds of tables – you don’t want to individually specify the same check for each.

- **Data Quality Dashboards and KPIs:** Higher management in enterprises often want a simple KPI to track (“what’s our data quality score?”). DQOps provides this via its **Data Quality KPI** computation and Looker Studio dashboards ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=DQOps%20measures%20data%20quality%20with,supports%20custom%20data%20quality%20dashboards)). Each check can contribute to a score (e.g., maybe all checks passing = 100% quality). DQOps can roll these up by table, by data source, or overall. The dashboards show trends, like moving from 85% to 90% as issues get resolved, which is a powerful way to demonstrate improvements to non-technical stakeholders. The fact that DQOps offers a *complimentary data quality warehouse and dashboards* in the cloud for free is very appealing ([Review data quality results on data quality dashboards](https://dqops.com/docs/getting-started/review-results-on-dashboards/#:~:text=Complimentary%20data%20quality%20data%20warehouse,for%20FREE%20accounts)) – it reduces the setup cost for getting management-level reporting. Enterprises that are sensitive about data egress might not use the cloud component, but they could replicate a similar setup internally since the data is accessible. Either way, the presence of a ready-made BI layer for data quality is a plus over tools that would require building that reporting yourself.

- **Enterprise Support and Extensibility:** While not a feature per se, it’s worth noting DQOps has a company (Documati) behind it, which likely offers support and perhaps an enterprise edition in the future. This means risk-averse enterprises can get professional support if needed. The platform being open-source also means it’s extensible – if something doesn’t quite fit, in-house teams could modify or contribute.

- **Modularity for CI/CD and Reusability:** The design of DQOps encourages reuse of checks definitions. For instance, if you create a custom check (sensor+rule) to validate a specific business logic, you can roll that out to multiple tables by just referencing it in their YAML. This is useful for enterprises where certain rules are globally applicable (e.g., any “email” column should satisfy an email regex check). You define it once, reuse everywhere. Moreover, because all interactions can be done through the API, an enterprise could even build an internal portal or integrate DQOps functions into a larger DataOps platform if they wanted. It doesn’t lock you into only using their UI – the openness via API is enterprise-friendly.

- **Deployment Flexibility (Cloud/On-Prem/Hybrid):** Enterprises can choose how to deploy DQOps. Some might run it fully on-premises (especially if data cannot leave their network) – that’s supported, you just run the engine locally and perhaps forego the cloud sync, or use a private cloud sync. Others might use DQOps Cloud (SaaS) for convenience, managing checks in the SaaS UI and agent. DQOps’s architecture page details possible SaaS, on-prem, and hybrid deployments ([DQOps Data Quality Engine Architecture](https://dqops.com/docs/dqo-concepts/architecture/dqops-architecture/#:~:text=This%20guide%20shows%20the%20components,premise%2C%20or%20hybrid%20environment)) ([DQOps Data Quality Engine Architecture](https://dqops.com/docs/dqo-concepts/architecture/dqops-architecture/#:~:text=DQOps%20is%20designed%20to%20support,user%20production%20environments)). This flexibility means enterprises can start quickly in the cloud (no infrastructure setup), then later migrate on-prem if required, with minimal change (since the config can be exported and reused).

In conclusion, DQOps was built with enterprise needs in mind: **automation, integration, scalability, and oversight**. Features like scheduling, notifications, YAML config, and dashboards address the full lifecycle of data quality management in a large organization – from defining rules, running them at scale, catching issues, alerting the right people, to providing high-level metrics on data health. All these capabilities come in the open-source package, which is a strong value proposition for enterprises comparing solutions. The comprehensive approach means a company can adopt DQOps and embed it in their data operations with relatively low friction, and have confidence that it will support growth in volume (big data) and breadth (many data sources and teams) ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=DQOps%20supports%20incremental%20data%20quality,on%20the%20monitored%20data%20source)) ([What is DQOps Data Quality Operations Center](https://dqops.com/docs/#:~:text=)).
